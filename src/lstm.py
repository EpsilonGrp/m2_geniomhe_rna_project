# -*- coding: utf-8 -*-
"""NewBaby.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZZRyYLNbjSJaKFWPvLoc-A7ydRBPYrSY

# LSTM for prediction of epsilon angles of RNA


## Method Used : Keras

## Import Libraries
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from numpy import array
from numpy import hstack
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from google.colab import files
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error

"""## Load Dataset"""

# Upload the CSV file
uploaded = files.upload()

# Assuming you uploaded 'training_with_classes.csv'
file_path = 'Final_trainingset.csv'
df = pd.read_csv(file_path)

"""Preprocessing"""

from sklearn.preprocessing import LabelEncoder
# Use LabelEncoder to encode 'name'
label_encoder = LabelEncoder()
df['name'] = label_encoder.fit_transform(df['name'])

df.head(10)

from sklearn.preprocessing import LabelEncoder

# Assuming 'class' is your target variable
target_variable = 'class'

# Create a label encoder
label_encoder = LabelEncoder()

# Apply label encoding to 'class' column
df[target_variable] = label_encoder.fit_transform(df[target_variable])

print(df.info())

df['class'].isnull().sum()

# Check the updated DataFrame
df.head(100)

"""# Notes :

input = fasta, epsilon

output = class

final prediction = angles
"""

input_features = ['A','C','G','U']
X = df[input_features].values

# Define n_features
n_features = len(input_features)

target_variable = 'class'
y = df[target_variable].values

"""# split a multivariate sequence into samples"""

def split_sequences(sequences, n_steps):
	X, y = list(), list()

	for i in range(len(sequences)):
		# find the end of this subsequence
		end_ix = i + n_steps
		# check if we are beyond the sequence
		if end_ix > len(sequences)-1:
			break
		# combine input and output parts of the subsequence
		seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)

def split_sequences(sequences, n_steps):
    X, y = list(), list()

    for i in range(len(sequences)):
        # find the end of this subsequence
        end_ix = i + n_steps
        # check if we are beyond the sequence
        if end_ix > len(sequences)-1:
            break
        # combine input and output parts of the subsequence
        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X), np.array(y)

"""# Split the dataset into train and test sets"""

# Assuming 'class' is your target variable
target_variable = 'class'
y = df[target_variable].values

# Reshape y to be 2D: (samples, features)
y = y.reshape(-1, 1)

# Data splitting
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Replace non-numeric values in y_train with the mean of the column
for i in range(y_train.shape[1]):
    column = y_train[:, i]
    non_numeric_mask = ~np.isnan(pd.to_numeric(column, errors='coerce'))
    if non_numeric_mask.any():
        mean_value = pd.to_numeric(column, errors='coerce').mean()
        y_train[:, i] = np.nan_to_num(pd.to_numeric(column, errors='coerce'), nan=mean_value)

# Replace non-numeric values in y_test with the mean of the column
for i in range(y_test.shape[1]):
    column = y_test[:, i]
    non_numeric_mask = ~np.isnan(pd.to_numeric(column, errors='coerce'))
    if non_numeric_mask.any():
        mean_value = pd.to_numeric(column, errors='coerce').mean()
        y_test[:, i] = np.nan_to_num(pd.to_numeric(column, errors='coerce'), nan=mean_value)

"""## Creating the Model
** You can apply the different deep learning tools that allow to improve the results **

Built a LSTM model with 1 hidden layer.

Every LSTM layer should be accompanied by a Dropout layer. This layer will help to prevent overfitting.
"""

# Reshape y to be 2D: (samples, features)
y = df[target_variable].values
y = y.reshape(-1, 1)

# Convert 'Na' or other non-numeric values to NaN and then fill with mean
y_numeric = pd.to_numeric(y.flatten(), errors='coerce')
y_numeric = np.nan_to_num(y_numeric, nan=np.nanmean(y_numeric))

# Reshape back to 2D
y_numeric = y_numeric.reshape(-1, 1)

# Data splitting
x_train, x_test, y_train, y_test = train_test_split(X, y_numeric, test_size=0.3, random_state=42)

# Standard Scaling for x_train and x_test
scaler_x = StandardScaler()
x_train_scaled = scaler_x.fit_transform(x_train)
x_test_scaled = scaler_x.transform(x_test)

# Standard Scaling for y_train and y_test
scaler_y = StandardScaler()
y_train_scaled = scaler_y.fit_transform(y_train)
y_test_scaled = scaler_y.transform(y_test)

# Reshape x_train and x_test to be 3D: (samples, time_steps, features)
x_train_reshaped = x_train_scaled.reshape((x_train_scaled.shape[0], 1, x_train_scaled.shape[1]))
x_test_reshaped = x_test_scaled.reshape((x_test_scaled.shape[0], 1, x_test_scaled.shape[1]))

# Define the model
model = Sequential()
model.add(LSTM(100, input_shape=(x_train_reshaped.shape[1], x_train_reshaped.shape[2])))
model.add(Dropout(0.2))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

# Fit the model
model.fit(x_train_reshaped, y_train_scaled, epochs=400, verbose=0)

"""## Model Evaluation"""

model.summary()

# Evaluate the model on the test set
score = model.evaluate(x_test_reshaped, y_test_scaled, verbose=1)
print("Scores mse", score)

# Predictions
trainPredict_scaled = model.predict(x_train_reshaped)
testPredict_scaled = model.predict(x_test_reshaped)

# Inverse transform the predictions
trainPredict = scaler_y.inverse_transform(trainPredict_scaled)
testPredict = scaler_y.inverse_transform(testPredict_scaled)

# Calculate MAE for training set
mae_train = mean_absolute_error(y_train.flatten(), trainPredict.flatten())
print(f"Mean Absolute Error (MAE) - Training Set: {mae_train}")

# Calculate MAE for test set
mae_test = mean_absolute_error(y_test.flatten(), testPredict.flatten())
print(f"Mean Absolute Error (MAE) - Testing Set: {mae_test}")

"""## Results Visualisation"""

# Plot baseline and predictions for the training set
plt.xlabel('Time')
plt.ylabel('Values')
plt.plot(y_train.flatten(), label='target')
plt.plot(trainPredict.flatten(), label='prediction')
plt.legend()
plt.show()

# Plot baseline and predictions for the test set
plt.xlabel('Time')
plt.ylabel('Values')
plt.plot(y_test.flatten(), label='target')
plt.plot(testPredict.flatten(), label='prediction')
plt.legend()
plt.show()

"""# Sample Test"""

# Get user input for sequence
sequence = input("Enter the sequence: ").upper()

# Convert the sequence string to a list of numeric codes
nucleotide_vector = list(sequence)

# Create a DataFrame
df2 = pd.DataFrame({'sequence': nucleotide_vector})

# One-hot encode nucleotides
df2['A'] = (df2['sequence'] == 'A').astype(int)
df2['C'] = (df2['sequence'] == 'C').astype(int)
df2['G'] = (df2['sequence'] == 'G').astype(int)
df2['U'] = (df2['sequence'] == 'U').astype(int)

# Rearrange columns
df2 = df2[['A', 'C', 'G', 'U']]

# Reshape the input sequence for prediction
x_input = df2[['A', 'C', 'G', 'U']].values.reshape(1, len(df2), len(input_features))

# Standard Scaling for x_input
x_input_scaled = scaler_x.transform(x_input.reshape(-1, len(input_features))).reshape(x_input.shape)

# Use the trained model to predict epsilon angles
predicted_angles_scaled = model.predict(x_input_scaled)

# Inverse transform the predicted angles
predicted_angles = scaler_y.inverse_transform(predicted_angles_scaled)

# Print the predicted angles
print("Predicted Epsilon Angles:")
print(predicted_angles)